import torch
from torch import nn
import cv2
import os

from PIL import Image
from torch.utils.data import DataLoader
from torchvision import transforms
from transformers import BlipProcessor, BlipForConditionalGeneration, CLIPProcessor, CLIPModel
from ultralytics import YOLO

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pickle
import torchvision



class YoloClipBlip():
    def __init__(self, device):
        self.device = device
        self.yolo = torch.hub.load('ultralytics/yolov5', 'yolov5l6', pretrained=True).to(device)
        self.clip = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device)
        self.clipProcessor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
        self.blip = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base").to(device)
        self.blipProcessor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")


    def _detect_objects(self, image, yolo):
        """
        args:
            image: input image,
            yolo: YOLOv5l6 pretrained model

        returns:
            a list of blurred_out images
        """
        images = []
        bboxes = []
        results = yolo(image)

        def crop(img, bbox):
            img = cv2.cvtColor(cv2.imread(img), cv2.COLOR_RGB2BGR)
            return img[int(bbox[1]):int(bbox[1])+int(bbox[3]), int(bbox[0]):int(bbox[0]+bbox[2])]

        for bbox in results.xyxy[0].cpu().numpy():
            bboxes.append(bbox)
            images.append(crop(image, bbox))

        return images, bboxes


    def _generate_captions(self, images, blip, blipProcessor, device):
        """
        args:
            images: list of blurred_out images
            blip: BLIP-base pretrained model
            blipProcessor: BLIP processor
            device: torch.device # GPU/CPU

        returns:
            a list of captions, one for each image
        """
        captions = []
        for image in images:
            processed_image = blipProcessor(image, return_tensors="pt").to(device)
            out = blip.generate(**processed_image).to(device)
            captions.append(blipProcessor.decode(out[0], skip_special_tokens=True))
        return captions


    def _preprocess(self, images, caption, clipProcessor, device):
        """
        args:
            images: list of blurred_out images
            caption: text from RefCOCOg
            clipProcessor: CLIP processor
            device: torch.device # GPU/CPU

        returns:
            preprocessed images/captions
        """
        return clipProcessor(text=caption, images=images, return_tensors="pt", padding=True).to(device)


    def _image_text_matching(self, clip, **inputs):
        """
        args:
            clip: CLIP ViT-B/32 pretrained model
            inputs: preprocessed images/caption

        returns:
            logits/matching scores between images and caption
        """
        outputs = clip(**inputs)
        return outputs.logits_per_text.softmax(dim=1)


    def _caption_similarity(self, caption, blipCaptions, clipProcessor, device):
        """
        args:
            caption: original caption from RefCOCOg
            blipCaptions: captions generated by BLIP-base
            device: torch.device # GPU/CPU

        returns:
            a list of cosine similarity scores between the dataset caption and blipCaptions clip embeddings
        """
        caption_embeddings = clipProcessor(text=caption, return_tensors="pt", padding=True).to(device)

        results = []

        from torch.nn import CosineSimilarity
        cos = CosineSimilarity()  # Use dim=1 to calculate cosine similarity along the batch dimension

        for blipCaption in blipCaptions:
            blipCaption_embeddings = clipProcessor(text=blipCaption, return_tensors="pt", padding=True).to(device)

            tensor1 = blipCaption_embeddings["input_ids"].to(torch.float32)
            tensor2 = caption_embeddings["input_ids"].to(torch.float32)

            max_len = max(tensor1.shape[1], tensor2.shape[1])
            tensor1 = torch.nn.functional.pad(tensor1, (0, max_len - tensor1.shape[1]))
            tensor2 = torch.nn.functional.pad(tensor2, (0, max_len - tensor2.shape[1]))

            similarity = cos(tensor1, tensor2)
            results.append(similarity.item())

        return results


    def visual_grounding(self, image, caption):
        """
        args:
            image: np.array
            caption: string

        returns:
            the best bounding box.
        """
        images, bboxes = self._detect_objects(image=image, yolo=self.yolo)
        if len(bboxes) == 0:
            return [0.00, 0.00, 0.00, 0.00]
        preprocessed = self._preprocess(images=images, caption=caption, clipProcessor=self.clipProcessor, device=self.device)
        logits = self._image_text_matching(self.clip, **preprocessed)
        blipCaptions = self._generate_captions(images=images, blip=self.blip, blipProcessor=self.blipProcessor, device=self.device)
        caption_similarity = self._caption_similarity(caption=caption, blipCaptions=blipCaptions, clipProcessor=self.clipProcessor, device=self.device)
        results = logits.cpu().detach().numpy() + caption_similarity
        best_bbox = bboxes[np.argmax(results[0])]

        return best_bbox

    def _iou(self, box1, box2):
        """Compute the intersection over union of two set of boxes, each box is [x1,y1,x2,y2]."""

        iou = torchvision.ops.box_iou(box1, box2)

        return iou

    def _get_metrics(self, box_pred, box_target):
        """Compute the accuracy, precision, recall and IoU."""
        mean_iou = []
        accuracy_0_3 = []
        accuracy_0_5 = []
        accuracy_0_75 = []
        conta = 0
        for b_t, b_p in zip(box_target, box_pred):

            IoU = self._iou(b_t.unsqueeze(0), b_p.unsqueeze(0))

            # get the iou
            # check if the IoU is between 0 and 1
            if IoU > -0.0001 and IoU < 1.0001:
                mean_iou.append(IoU)

                # get the accuracy
                if IoU > 0.3:
                    accuracy_0_3.append(1)
                else:
                    accuracy_0_3.append(0)

                if IoU > 0.5:
                    accuracy_0_5.append(1)
                else:
                    accuracy_0_5.append(0)

                if IoU > 0.75:
                    accuracy_0_75.append(1)
                else:
                    accuracy_0_75.append(0)

            else:
                pass

        # get the mean iou
        mean_iou_m = torch.stack(mean_iou).mean().numpy()

        # get the accuracy
        accuracy_0_3_m = np.array(accuracy_0_3).mean()
        accuracy_0_5_m = np.array(accuracy_0_5).mean()
        accuracy_0_75_m = np.array(accuracy_0_75).mean()

        # get precision
        precision_0_3 = np.array(accuracy_0_3).sum() / len(accuracy_0_3)
        precision_0_5 = np.array(accuracy_0_5).sum() / len(accuracy_0_5)
        precision_0_75 = np.array(accuracy_0_75).sum() / len(accuracy_0_75)

        # get recall
        recall_0_3 = np.array(accuracy_0_3).sum() / len(box_target)
        recall_0_5 = np.array(accuracy_0_5).sum() / len(box_target)
        recall_0_75 = np.array(accuracy_0_75).sum() / len(box_target)

        # get f1
        f1_0_3 = 2 * (precision_0_3 * recall_0_3) / (precision_0_3 + recall_0_3)
        f1_0_5 = 2 * (precision_0_5 * recall_0_5) / (precision_0_5 + recall_0_5)
        f1_0_75 = 2 * (precision_0_75 * recall_0_75) / (precision_0_75 + recall_0_75)


        return mean_iou_m, accuracy_0_3_m, accuracy_0_5_m, accuracy_0_75_m, precision_0_3, precision_0_5, precision_0_75, recall_0_3, recall_0_5, recall_0_75, f1_0_3, f1_0_5, f1_0_75

    def _get_dict_with_scores(self, mean_iou_m, accuracy_0_3_m, accuracy_0_5_m, accuracy_0_75_m, precision_0_3, precision_0_5, precision_0_75, recall_0_3, recall_0_5, recall_0_75, f1_0_3, f1_0_5, f1_0_75):
        """Return a dictionary with the scores"""
        return {
                'mean_iou_m': mean_iou_m.round(2),
                'accuracy_0_3_m': accuracy_0_3_m.round(2),
                'accuracy_0_5_m': accuracy_0_5_m.round(2),
                'accuracy_0_75_m': accuracy_0_75_m.round(2),
                'precision_0_3': precision_0_3.round(2),
                'precision_0_5': precision_0_5.round(2),
                'precision_0_75': precision_0_75.round(2),
                'recall_0_3': recall_0_3.round(2),
                'recall_0_5': recall_0_5.round(2),
                'recall_0_75': recall_0_75.round(2),
                'f1_0_3': f1_0_3.round(2),
                'f1_0_5': f1_0_5.round(2),
                'f1_0_75': f1_0_75.round(2)
            }


    def evaluation(self, dataset):

        from tqdm import tqdm

        best_bboxes = []
        refcocog_bboxes = []

        # Use tqdm to create a progress bar
        for d in tqdm(dataset, desc="Evaluating dataset", unit="image"):
            best_bboxes.append(self.visual_grounding(image=d["file_name"], caption=d["caption"]))
            refcocog_bboxes.append(d["bbox"])

        # save bboxes for faster evaluation afterwards
        np.save('./best_bboxes.npy', np.stack(np.array(best_bboxes)))
        np.save('./refcocog_bboxes.npy', np.stack(np.array(refcocog_bboxes)))

        # get the metrics
        mean_iou_m, accuracy_0_3_m, accuracy_0_5_m, accuracy_0_75_m, precision_0_3, precision_0_5, precision_0_75, recall_0_3, recall_0_5, recall_0_75, f1_0_3, f1_0_5, f1_0_75 = self._get_metrics(torch.from_numpy(best_bboxes), torch.from_numpy(refcocog_bboxes))

        # get the dictionary with the scores
        dict_with_scores = self._get_dict_with_scores(self, mean_iou_m, accuracy_0_3_m, accuracy_0_5_m, accuracy_0_75_m, precision_0_3, precision_0_5, precision_0_75, recall_0_3, recall_0_5, recall_0_75, f1_0_3, f1_0_5, f1_0_75)

        # convert the dictionary to a dataframe
        df = pd.DataFrame.from_dict(dict_with_scores, orient='index').T

        return df
